{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"Tt2OChN115Ba","executionInfo":{"status":"error","timestamp":1670890984809,"user_tz":300,"elapsed":165,"user":{"displayName":"Max Hannon","userId":"11625285732207788632"}},"outputId":"a8a85de7-6f74-4f6b-9b4f-4580c53e0bc5"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-6025c263e81f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import csv\n","from transformers import BertModel, BertTokenizer, AutoTokenizer\n","import torch\n","import numpy\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression as sklearn_LR\n","\n","model_name = 'bert-base-uncased'"]},{"cell_type":"markdown","metadata":{"id":"1M6bSTN515Bd"},"source":["**The following cell uses the load data function defined in the LogisticRegression file provided by the course staff of CS490A @Umass Amherst. It is used to extract the texts and labels from each data split**"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"0zD2DGYy15Bf","executionInfo":{"status":"ok","timestamp":1670891013176,"user_tz":300,"elapsed":169,"user":{"displayName":"Max Hannon","userId":"11625285732207788632"}}},"outputs":[],"source":["# code provided by CS490A course staff\n","\n","def load_data(filename):\n","    '''\n","    Method for loading in data from tsv.\n","    \n","    Input:\n","    - filename (str): tsv file containing annotated data must include\n","                      header with the named columns \"label\" and \"text\"\n","\n","    Returns a lists of strings corresponding to the document texts\n","    and labels respectively\n","    '''\n","    with open(filename, encoding=\"utf-8\", newline='') as tsv:\n","        reader = csv.DictReader(tsv, delimiter='\\t')\n","        \n","        texts, labels = [], []\n","        for row in reader:\n","            texts.append(row['text'])\n","            labels.append(row['label'])\n","\n","    return texts, labels"]},{"cell_type":"markdown","metadata":{"id":"zrg3lGOz15Bh"},"source":["**Here we used the load data function to extract texts and labels from each split**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybccfK9v15Bh"},"outputs":[],"source":["train_texts, train_labels = load_data(\"/Users/seanmurphy/Downloads/AP4 Supporting Notebooks-20221210/splits/train.tsv\")\n","test_texts, test_labels = load_data(\"/Users/seanmurphy/Downloads/AP4 Supporting Notebooks-20221210/splits/test.tsv\")\n","dev_texts, dev_labels = load_data(\"/Users/seanmurphy/Downloads/AP4 Supporting Notebooks-20221210/splits/dev.tsv\")"]},{"cell_type":"markdown","metadata":{"id":"9jGU4Uae15Bi"},"source":["**Here we define Bert model and tokenizer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5_fIV1p15Bi","outputId":"2be268bd-eb82-48ec-cf14-51847dc96f84"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["model_name = 'bert-base-uncased'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n"]},{"cell_type":"markdown","metadata":{"id":"ujIcqaPc15Bk"},"source":["**The following cell is a function provided by the course staff of CS490A @Umass Amherst. It used to get confidence intervals for given classifications**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2IHTrTMm15Bl"},"outputs":[],"source":["# code provided by CS490A course staff\n","\n","import scipy\n","\n","def get_confidence_intervals(accuracy, sample_size, confidence_level):\n","  z_score = -1 * scipy.stats.norm.ppf((1-confidence_level)/2)\n","  standard_error = numpy.sqrt(accuracy * (1-accuracy) / sample_size)\n","  lower_ci = accuracy - standard_error*z_score\n","  upper_ci = accuracy + standard_error*z_score\n","  return lower_ci, upper_ci"]},{"cell_type":"markdown","metadata":{"id":"d108iMzj15Bm"},"source":["**The following function are used to extract to binary features from the documents. The first is whether or not the question contains a blank line _____. This is because most questions which contain a black line are sports and leisure quetions. The second checks if their is a date present in the question by matching 3-4 adjacent digits. Questions with dates are usually History.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnVLHGgb15Bn"},"outputs":[],"source":["import re\n","\n","# check if a questions contains a fill in the blank. If this is the case it is almost certainly Sports and Leisure\n","def contains_line(doc):\n","    if (\"_______\" in doc):\n","        return 1\n","    else:\n","        return 0\n","\n","# check if a question contains a date. If this is the case, it is likely History\n","def contains_date(doc):\n","    if (len(re.findall(\"[0-9]{3,4}\", doc))>0):\n","        return 1\n","    else:\n","        return 0"]},{"cell_type":"markdown","metadata":{"id":"2mH3zMWi15Bn"},"source":["**Here is a function partially created in HW4 of the course. This has been modified from it's original state. The function is used to extract the hidden layers provided by BERT. It also adds two binary features to the end of each hidden layer vector (the two features defined above)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVgGCPFa15Bo"},"outputs":[],"source":["def extract_bert_features(input_texts):\n","  features = []\n","  for i, text in enumerate(input_texts):\n","    input = tokenizer.encode(text, truncation=True,\n","                             return_tensors=\"pt\")\n","    hidden_states = model(input).hidden_states\n","    feature = torch.stack([hidden_state[0][0] for hidden_state in hidden_states[1:]]).detach().cpu().numpy()\n","    assert feature.shape == (12, 768)\n","\n","    new_features = []\n","    for layer in range(12):\n","      new_features.append(numpy.append(numpy.append(feature[layer], contains_line(text)), contains_date(text)))\n","      \n","    new_features = numpy.stack(new_features)\n","    features.append(new_features)\n","  return numpy.stack(features)"]},{"cell_type":"markdown","metadata":{"id":"WeHMk1my15Bp"},"source":["**Here we extract the train, test, and dev embeddings to use as features in the future**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAzUy97w15Bp"},"outputs":[],"source":["train_features = extract_bert_features(train_texts)\n","test_features = extract_bert_features(test_texts)\n","dev_features = extract_bert_features(dev_texts)\n"]},{"cell_type":"markdown","metadata":{"id":"yFFOQDj215Bq"},"source":["**The following cell is code provided by the CS490A course staff. It creates a class for a logistic regression classifier**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwvqETGx15Bq"},"outputs":[],"source":["# code provided by CS490A course staff\n","\n","class Classifier:\n","    '''\n","    A l2-regularized logistic regression classifier \n","    '''\n","    def __init__(self):\n","        self.model = None\n","        self.C = -1\n","\n","    def train_and_optimize(self, train_vectors, train_labels,\n","                            dev_vectors, dev_labels):\n","        assert not self.model, \"ERROR: Model already trained.\"\n","        best_C = -1\n","        best_score = 0\n","        best_model = None\n","        for C in [0.1, 1, 10, 100]:\n","            model = sklearn_LR(C=C, max_iter=1000)\n","            model.fit(train_vectors, train_labels)\n","            train_acc = model.score(train_vectors, train_labels)\n","            dev_acc = model.score(dev_vectors, dev_labels)\n","            print(\"C = {} - Train Accuracy: {:.3f}, Dev Accuracy: {:.3f}\".format(\n","                  C, train_acc, dev_acc))\n","            if dev_acc > best_score:\n","                best_C = C\n","                best_model = model\n","                best_score = dev_acc\n","        print(\"Selected C = {}\".format(best_C))\n","        self.C = best_C\n","        self.model = best_model\n","    \n","    def predict(self, vectors):\n","        assert self.model, \"ERROR: Must train model first\"\n","        return self.model.predict(vectors)\n","\n","    def test(self, test_vectors, test_labels):\n","        assert self.model, \"ERROR: Must train model first\"\n","        # Compute test accuracy\n","        accuracy = self.model.score(test_vectors, test_labels)\n","        n = test_vectors.shape[0]\n","        \n","        # Compute 95% confidence interval using normal approximation\n","        confidence_level = 0.95\n","        z_score = -1 * scipy.stats.norm.ppf((1-confidence_level)/2)\n","        standard_error = numpy.sqrt(accuracy * (1-accuracy) / n)\n","        lower_ci = accuracy - standard_error*z_score\n","        upper_ci = accuracy + standard_error*z_score\n","        print(\"Test Accuracy: {:.3f} with 95% CI: [{:.3f}, {:.3f}]\".format(\n","              accuracy, lower_ci, upper_ci))\n","\n","    def print_weights(self, display_k=5, feature_names=None):\n","        assert self.model, \"ERROR: Must train model first\"\n","        \n","        n_classes = len(self.model.classes_)\n","        # binary (2 label classes)\n","        if n_classes == 2:\n","            weights = self.model.coef_[0]\n","            sorted_idx = numpy.argsort(weights)\n","            \n","            # positive class\n","            label = self.model.classes_[1]\n","            for i in sorted_idx[-display_k:][::-1]:\n","                weight = weights[i]\n","                if weight <= 0:\n","                    continue\n","                feat = feature_names[i] if feature_names else i\n","                print(\"{}\\t{}\\t{:.3f}\".format(label, feat, weight))\n","            print()\n","            \n","            # negtaive class\n","            label = self.model.classes_[0]\n","            for i in sorted_idx[:display_k]:\n","                weight = weights[i]\n","                if weight >= 0:\n","                    continue\n","                feat = feature_names[i] if feature_names else i\n","                print(\"{}\\t{}\\t{:.3f}\".format(label, feat, weight))\n","        \n","        # mulitclass (3+ label classes)\n","        else:\n","            for i, label in enumerate(self.model.classes_):\n","                weights = self.model.coef_[i]\n","                sorted_idx = numpy.argsort(weights)\n","                for i in sorted_idx[-display_k:][::-1]:\n","                    weight = weights[i]\n","                    feat = feature_names[i] if feature_names else i\n","                    print(\"{}\\t{}\\t{:.3f}\".format(label, feat, weight))\n","                print()"]},{"cell_type":"markdown","metadata":{"id":"AiwhnfNa15Bs"},"source":["**The following code is used to extract a specific hidden layer from each split**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5HqiO7P15Bs"},"outputs":[],"source":["def get_layer(i):\n","    return {\"train\": train_features[:, i-1, :], \"test\": test_features[:, i-1, :], \"dev\": dev_features[:, i-1, :]}"]},{"cell_type":"markdown","metadata":{"id":"O3BiABz-15Bt"},"source":["**Here we extract layer 11 from each split (it had the best perfoormance of all layers). Then we instantiate a classifier and use the code provided to optimize. Lastly the model is evaluated on the test set.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oac-suu615Bt","outputId":"e386a0d5-6e90-4d1f-c297-e12a07bfd6c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training & Optimizing Model...\n","C = 0.1 - Train Accuracy: 0.950, Dev Accuracy: 0.765\n","C = 1 - Train Accuracy: 1.000, Dev Accuracy: 0.795\n","C = 10 - Train Accuracy: 1.000, Dev Accuracy: 0.780\n","C = 100 - Train Accuracy: 1.000, Dev Accuracy: 0.775\n","Selected C = 1\n","\n","Evaluating Model...\n","Test Accuracy: 0.800 with 95% CI: [0.745, 0.855]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}],"source":["\n","# Featurize texts\n","feature_data = get_layer(11)\n","\n","print(\"\\nTraining & Optimizing Model...\")\n","# Train model\n","model = Classifier()\n","model.train_and_optimize(feature_data[\"train\"], train_labels,\n","                            feature_data[\"dev\"], dev_labels\n","                            )\n","print(\"\\nEvaluating Model...\")\n","# Evaluate Model\n","model.test(feature_data[\"test\"], test_labels)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}